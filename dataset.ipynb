{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f5d31-fdf5-4b7f-a8dd-77642ffed6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "\n",
    "import import_ipynb\n",
    "from simulation import simul_multi, simul\n",
    "from constants import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaf214-33c4-450b-8255-b59fdc3a0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tag must start with '-'\n",
    "def generate_dataset(graph_name, data_num, saving_tag='', print_epoch=100):\n",
    "    n,m,adj = txt2adj(graph_name)\n",
    "\n",
    "    data = []    \n",
    "    seed_sizes = np.random.randint(min(10,n), max(10,int(n*SEED_SIZE)), size=data_num)\n",
    "    for i in range(data_num):\n",
    "        if i%print_epoch==0: print(f'{i}/{data_num}th simulation start')\n",
    "        seed_idx = np.random.choice(n,seed_sizes[i],replace=False)\n",
    "        is_seed = np.zeros(n, dtype=int)\n",
    "        is_seed[seed_idx] = 1\n",
    "        \n",
    "        prob = simul_multi(adj,seed_idx)\n",
    "\n",
    "        while prob.sum().item()==seed_sizes[i]:\n",
    "            seed_idx = np.random.choice(n,seed_sizes[i],replace=False)\n",
    "            is_seed = np.zeros(n, dtype=int)\n",
    "            is_seed[seed_idx] = 1\n",
    "            \n",
    "            prob = simul_multi(adj,seed_idx)\n",
    "        \n",
    "        data.append((is_seed,prob))\n",
    "    \n",
    "    with gzip.open(DATASET_DIR+graph_name+saving_tag+'.pkl.gz','wb') as f: pickle.dump(data, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badcb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# generate_dataset('Celebrity_test_LP', 50, saving_tag='-50_new', print_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445496f-1e2c-4e2a-9c0e-7fee95d6fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_name, data_num=None):\n",
    "    graph_name = dataset_name.split('-')[0]\n",
    "    edge_index, edge_attr = txt2coo(graph_name)\n",
    "    \n",
    "    data = []\n",
    "    with gzip.open(DATASET_DIR+dataset_name, 'rb') as f: rawdata = pickle.load(f)\n",
    "    if data_num==None: data_num = len(rawdata)\n",
    "    for is_seed, prob in rawdata[:data_num]:\n",
    "        is_seed = torch.from_numpy(np.expand_dims(is_seed,axis=-1)).float()\n",
    "        prob = torch.from_numpy(np.expand_dims(prob,axis=-1)).float()\n",
    "        G = Data(x=is_seed, edge_index=edge_index, edge_attr=edge_attr, y=prob)\n",
    "        data.append(G)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data2(dataset_name1, dataset_name2, data_num=None):\n",
    "    data1 = get_data(dataset_name1, data_num)\n",
    "    data2 = get_data(dataset_name2, data_num)\n",
    "    data = data1+data2\n",
    "    random.shuffle(data)\n",
    "    del data1\n",
    "    del data2\n",
    "    return data\n",
    "\n",
    "    \n",
    "def get_data_split(dataset_name, data_num=None):\n",
    "    data = get_data(dataset_name, data_num)\n",
    "    \n",
    "    train_num = int(len(data)*0.8)\n",
    "    val_num = int(len(data)*0.1)\n",
    "    test_num = len(data)-train_num-val_num\n",
    "\n",
    "    train_data = data[:train_num]\n",
    "    val_data = data[train_num:train_num+val_num]\n",
    "    test_data = data[train_num+val_num:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def get_data_split2(dataset_name1, dataset_name2, data_num=None):\n",
    "    data = get_data2(dataset_name1, dataset_name2, data_num)\n",
    "    \n",
    "    train_num = int(len(data)*0.8)\n",
    "    val_num = int(len(data)*0.1)\n",
    "    test_num = len(data)-train_num-val_num\n",
    "\n",
    "    train_data = data[:train_num]\n",
    "    val_data = data[train_num:train_num+val_num]\n",
    "    test_data = data[train_num+val_num:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def get_dataloader(dataset_name, data_num=None, batch_size=20):\n",
    "    train_data, val_data, test_data = get_data_split(dataset_name, data_num)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def get_dataloader2(dataset_name1, dataset_name2, data_num=None, batch_size=32):\n",
    "    train_data, val_data, test_data = get_data_split2(dataset_name1, dataset_name2, data_num)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
