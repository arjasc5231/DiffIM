{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627466e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import import_ipynb\n",
    "from dataset import get_data\n",
    "from constants import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8013480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train GNN\n",
    "\n",
    "def train(train_dataset_name, test_dataset_name, hyper_params={}, pre_ckpt_name=None, saving_name=None, gpu_num='cpu', print_epoch=0):\n",
    "    if saving_name and '.pt' not in saving_name : saving_name+='.pt'\n",
    "    record = []\n",
    "\n",
    "    # initialize gpu\n",
    "    torch.cuda.empty_cache()\n",
    "    device = set_gpu(gpu_num)\n",
    "    \n",
    "    # set hyper_params\n",
    "    max_epoch = hyper_params.get('max_epoch', 2000)\n",
    "    lr = hyper_params.get('lr', 0.002)\n",
    "    lr_gamma = hyper_params.get('lr_gamma', 0.999)\n",
    "    gnn_latent_dim = hyper_params.get('gnn_latent_dim', [128,128,128,128,128,128])\n",
    "    \n",
    "    # load data\n",
    "    data = get_data(train_dataset_name)\n",
    "    train_num = int(len(data)*0.8)\n",
    "    train_data = data[:train_num]\n",
    "    val_data = data[train_num:]\n",
    "    test_data = get_data(test_dataset_name)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # load model\n",
    "    model = load_model(pre_ckpt_name, device, gnn_latent_dim=gnn_latent_dim)\n",
    "\n",
    "    # training setup\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: lr_gamma)\n",
    "\n",
    "    best_val_loss_graph = float('inf')\n",
    "    best_model_epoch = 0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    stime = time.time()\n",
    "    if print_epoch: print('epoch \\t train_loss_n \\t train_loss_g \\t val_loss_n \\t val_loss_g')\n",
    "    for epoch in range(1,max_epoch+1):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss_node = 0\n",
    "        train_loss_graph = 0\n",
    "        train_prediction_errp = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            preds = model(batch, node_regression=True)\n",
    "            \n",
    "            loss = loss_fn(preds,batch.y)\n",
    "            train_loss_node += loss.item()\n",
    "            \n",
    "            preds = scatter(preds, batch.batch, dim=0, reduce='sum')\n",
    "            labels = scatter(batch.y, batch.batch, dim=0, reduce='sum')\n",
    "            fracs = preds/labels\n",
    "            ones = torch.ones(fracs.size(), device=device)\n",
    "            train_loss_graph += loss_fn(preds,labels).item()\n",
    "            train_prediction_errp += loss_fn(ones,fracs).item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        train_loss_node /= len(train_dataloader.dataset)\n",
    "        train_loss_graph /= len(train_dataloader.dataset)\n",
    "        train_prediction_errp /= len(train_dataloader.dataset)\n",
    "        train_loss_node = train_loss_node**0.5\n",
    "        train_loss_graph = train_loss_graph**0.5\n",
    "        train_prediction_errp = train_prediction_errp**0.5\n",
    "    \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_node = 0\n",
    "            val_loss_graph = 0\n",
    "            val_prediction_errp = 0\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                \n",
    "                preds = model(batch, node_regression=True)\n",
    "                \n",
    "                loss = loss_fn(preds,batch.y)\n",
    "                val_loss_node += loss.item()\n",
    "\n",
    "                preds = scatter(preds, batch.batch, dim=0, reduce='sum')\n",
    "                labels = scatter(batch.y, batch.batch, dim=0, reduce='sum')\n",
    "                fracs = preds/labels\n",
    "                ones = torch.ones(fracs.size(),device=device)\n",
    "                val_loss_graph += loss_fn(preds,labels).item()\n",
    "                val_prediction_errp += loss_fn(ones,fracs).item()\n",
    "            val_loss_node /= len(val_dataloader.dataset)\n",
    "            val_loss_graph /= len(val_dataloader.dataset)\n",
    "            val_prediction_errp /= len(val_dataloader.dataset)\n",
    "            val_loss_node = val_loss_node**0.5\n",
    "            val_loss_graph = val_loss_graph**0.5\n",
    "            val_prediction_errp = val_prediction_errp**0.5\n",
    "    \n",
    "        # record best weight\n",
    "        if best_val_loss_graph > val_loss_graph:\n",
    "            best_val_loss_graph = val_loss_graph\n",
    "            best_model_epoch = epoch\n",
    "            best_model_state_dict = model.state_dict()\n",
    "        \n",
    "        record.append((train_loss_node, train_loss_graph, train_prediction_errp, val_loss_node, val_loss_graph, val_prediction_errp))\n",
    "        if print_epoch and epoch%print_epoch==0 : print(f'{epoch} \\t {train_loss_node:.6f} \\t {train_loss_graph:.6f} \\t {val_loss_node:.6f} \\t {val_loss_graph:.6f}')\n",
    "\n",
    "        scheduler.step()\n",
    "    training_time = time.time()-stime\n",
    "    if print_epoch: print('training end. time:', training_time)\n",
    "    \n",
    "    model.load_state_dict(best_model_state_dict)  # load best weight\n",
    "    if saving_name : torch.save(best_model_state_dict, MODEL_DIR+saving_name)  # save best weight\n",
    "            \n",
    "    # test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss_node = 0\n",
    "        test_loss_graph = 0\n",
    "        for batch in test_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            preds = model(batch, node_regression=True)\n",
    "            \n",
    "            loss = loss_fn(preds,batch.y)\n",
    "            test_loss_node += loss.item()\n",
    "            \n",
    "            preds = scatter(preds, batch.batch, dim=0, reduce='sum')\n",
    "            labels = scatter(batch.y, batch.batch, dim=0, reduce='sum')\n",
    "            test_loss_graph += loss_fn(preds,labels).item()\n",
    "        test_loss_node /= len(test_dataloader.dataset)\n",
    "        test_loss_graph /= len(test_dataloader.dataset)\n",
    "        test_loss_node = test_loss_node**0.5\n",
    "        test_loss_graph = test_loss_graph**0.5\n",
    "    if print_epoch: \n",
    "        print('test loss_n:', test_loss_node)\n",
    "        print('test loss_g:', test_loss_graph)\n",
    "        print()\n",
    "\n",
    "    if saving_name:\n",
    "        with open(RESULT_DIR+'model_train_log/'+saving_name[:-3]+'.txt','w') as f:\n",
    "            f.write(f'time: {training_time}, test_loss_graph: {test_loss_graph}, test_loss_node: {test_loss_node}, best_model_epoch: {best_model_epoch}\\n')\n",
    "            f.write('epoch \\t train_loss_n \\t train_loss_g \\t train_errp \\t val_loss_n \\t val_loss_g \\t val_errp\\n')\n",
    "            for i, (train_loss_node, train_loss_graph, train_prediction_errp, val_loss_node, val_loss_graph, val_prediction_errp) in enumerate(record):\n",
    "                f.write(f'{i+1} {train_loss_node} {train_loss_graph} {train_prediction_errp} {val_loss_node} {val_loss_graph} {val_prediction_errp}\\n')\n",
    "\n",
    "    return best_val_loss_graph, record, test_loss_graph, test_loss_node, best_model_epoch, best_model_state_dict, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9714370f-42cb-45d7-ab77-535d06ec8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as test part of train()\n",
    "\n",
    "def test(model_name, dataset_name, verbose=False, gpu_num='cpu'):\n",
    "    device = set_gpu(gpu_num)\n",
    "    model = load_model(model_name, device)  \n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    data = get_data(dataset_name)\n",
    "    dataloader = DataLoader(data, batch_size=20, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss_node = 0\n",
    "        test_loss_graph = 0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            preds = model(batch, node_regression=True)\n",
    "            \n",
    "            loss = loss_fn(preds,batch.y)\n",
    "            test_loss_node += loss.item()\n",
    "\n",
    "            preds = scatter(preds, batch.batch, dim=0, reduce='sum')\n",
    "            labels = scatter(batch.y, batch.batch, dim=0, reduce='sum')\n",
    "            seeds = scatter(batch.x, batch.batch, dim=0, reduce='sum')\n",
    "            \n",
    "            test_loss_graph += loss_fn(preds,labels).item()\n",
    "            \n",
    "            if verbose:\n",
    "                for i in range(len(preds)): print(seeds[i].item(), labels[i].item(), preds[i].item())\n",
    "        test_loss_node /= len(dataloader.dataset)\n",
    "        test_loss_graph /= len(dataloader.dataset)\n",
    "        test_loss_node = test_loss_node**0.5\n",
    "        test_loss_graph = test_loss_graph**0.5\n",
    "    print('test loss(node level):', test_loss_node)\n",
    "    print('test loss(graph level):', test_loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6267cc-5344-464f-b916-0c1fd49bbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning by Optuna\n",
    "\n",
    "def hparam_tuning(*args, **kwargs):\n",
    "    if 'saving_name' in kwargs:\n",
    "        saving_name = kwargs['saving_name']\n",
    "        if saving_name and '.pt' not in saving_name : saving_name+='.pt'\n",
    "        del kwargs['saving_name']\n",
    "    if 'hyper_params' in kwargs:\n",
    "        hyper_params = kwargs['hyper_params']\n",
    "        del kwargs['hyper_params']\n",
    "    else: hyper_params = {}\n",
    "    \n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_float('lr',1e-3, 1e-1, log=True)\n",
    "        lr_gamma = trial.suggest_float('lr_gamma',0.99, 0.999, log=True)\n",
    "        new_hyper_params = {'lr': lr, 'lr_gamma': lr_gamma}\n",
    "        new_hyper_params.update(hyper_params)\n",
    "        \n",
    "        ret = train(*args, **kwargs, hyper_params=new_hyper_params)\n",
    "        trial.set_user_attr(\"ret\", ret)\n",
    "        \n",
    "        return ret[0]\t\n",
    "    \n",
    "    study = optuna.create_study(sampler=TPESampler(n_startup_trials=5))\n",
    "    study.optimize(objective, n_trials=15)\n",
    "\n",
    "    print(f'best_trial value : {study.best_trial.value}') \n",
    "    print(f'best_params : {study.best_params}')\n",
    "    \n",
    "    if saving_name:\n",
    "        best_val_loss_graph, record, test_loss_graph, test_loss_node, best_model_epoch, best_model_state_dict, training_time = study.best_trial.user_attrs[\"ret\"]\n",
    "        lr, lr_gamma = study.best_params['lr'], study.best_params['lr_gamma']\n",
    "        torch.save(best_model_state_dict, MODEL_DIR+saving_name)\n",
    "        with open(RESULT_DIR+'model_train_log/'+saving_name[:-3]+'.txt','w') as f:\n",
    "            f.write(f'time: {training_time}, lr: {lr}, lr_gamma: {lr_gamma}\\n')\n",
    "            f.write(f'best_model_epoch: {best_model_epoch}, best_val_loss_graph: {best_val_loss_graph}\\n')\n",
    "            f.write(f'test_loss_graph: {test_loss_graph}, test_loss_node: {test_loss_node}\\n')\n",
    "            f.write('epoch \\t train_loss_n \\t train_loss_g \\t train_errp \\t val_loss_n \\t val_loss_g \\t val_errp\\n')\n",
    "            for i, (train_loss_node, train_loss_graph, train_prediction_errp, val_loss_node, val_loss_graph, val_prediction_errp) in enumerate(record): f.write(f'{i+1} {train_loss_node} {train_loss_graph} {train_prediction_errp} {val_loss_node} {val_loss_graph} {val_prediction_errp}\\n')\n",
    "        with gzip.open(RESULT_DIR+'study_obj/'+saving_name[:-3]+'.pkl.gz','wb') as f: pickle.dump(study, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence estimation quality\n",
    "\n",
    "def evaluate_quality(model_name, gnn_latent_dim, train_dataset_name, gpu_num):\n",
    "    device = set_gpu(gpu_num)\n",
    "\n",
    "    # load data\n",
    "    data = get_data(train_dataset_name)\n",
    "    train_num = int(len(data)*0.8)\n",
    "    val_data = data[train_num:]\n",
    "\n",
    "    # load model\n",
    "    model = load_model(model_name, device, gnn_latent_dim=gnn_latent_dim)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_data:\n",
    "            pred = model(data)[0][0].item()\n",
    "            label = np.sum(data.y.numpy())\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    sorted_indices = np.argsort(labels)\n",
    "    labels = labels[sorted_indices]\n",
    "    preds = preds[sorted_indices]\n",
    "\n",
    "    print('r =', np.corrcoef(preds, labels)[0,1])\n",
    "    print('MC GNN')\n",
    "    for i in range(len(preds)): print(labels[i], preds[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
