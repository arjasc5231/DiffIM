{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.explain import Explainer, CaptumExplainer\n",
    "\n",
    "import import_ipynb\n",
    "from dataset import get_data\n",
    "from constants import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RumorGuard_G(adj_list, seed_idx, prob, del_edge_num, model_name=None, gnn_latent_dim=[128,128,128,128,128,128], gpu_num='cpu', **kwargs):\n",
    "    time_start = time.time()\n",
    "    time_alg = []\n",
    "    \n",
    "    device = set_gpu(gpu_num)\n",
    "    model = load_model(model_name, device, gnn_latent_dim=gnn_latent_dim)\n",
    "\n",
    "    # convert adj_list to Data\n",
    "    data = adj2Data(adj_list, seed_idx)\n",
    "    data = data.to(device)\n",
    "    edge_index = data.edge_index\n",
    "    edge_attr = data.edge_attr\n",
    "    \n",
    "    mask = []\n",
    "    for _ in range(del_edge_num):\n",
    "        pred_min = float('inf')\n",
    "        idx_min = None\n",
    "        for i in range(len(edge_attr)):\n",
    "            u = edge_index[0,i]\n",
    "            v = edge_index[1,i]\n",
    "            p = edge_attr[i][0].item()\n",
    "            if p==0: continue\n",
    "            edge_attr[i][0] = 0\n",
    "            pred = model(data)[0].item()\n",
    "            edge_attr[i][0] = p\n",
    "            if pred<pred_min: pred_min = pred; idx_min=(u,v,i,p)\n",
    "        mask.append([idx_min[0],idx_min[1],idx_min[3]])\n",
    "        edge_attr[idx_min[2]]=0\n",
    "\n",
    "        time_alg.append(time.time()-time_start)\n",
    "        \n",
    "    return mask, time_alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RumorGuard_O is implemented based on GNNExplainer of PyG\n",
    "\n",
    "from math import sqrt\n",
    "from typing import Optional, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.explain import ExplainerConfig, Explanation, ModelConfig\n",
    "from torch_geometric.explain.algorithm import ExplainerAlgorithm\n",
    "from torch_geometric.explain.algorithm.utils import clear_masks, set_masks\n",
    "from torch_geometric.explain.config import MaskType, ModelMode, ModelTaskLevel\n",
    "\n",
    "\n",
    "class RumorGuard_O_class(ExplainerAlgorithm):\n",
    "\n",
    "    def __init__(self, epochs=100, lr=0.01, del_edge_num=10, size_coeff=1, ent_coeff=1, lr_gamma=1, log_filename=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.eps = 1e-15\n",
    "        self.del_edge_num = del_edge_num\n",
    "\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.size_coeff = size_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.log_filename = log_filename\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ) -> Explanation:\n",
    "        if isinstance(x, dict) or isinstance(edge_index, dict):\n",
    "            raise ValueError(f\"Heterogeneous graphs not yet supported in \"\n",
    "                             f\"'{self.__class__.__name__}'\")\n",
    "\n",
    "        self._train(model, x, edge_index, target=target, index=index, **kwargs)\n",
    "\n",
    "        node_mask = self._post_process_mask(\n",
    "            self.node_mask,\n",
    "            self.hard_node_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "        edge_mask = self._post_process_mask(\n",
    "            self.edge_mask,\n",
    "            self.hard_edge_mask,\n",
    "            apply_sigmoid=True,\n",
    "        )\n",
    "\n",
    "        self._clean_model(model)\n",
    "\n",
    "        return Explanation(node_mask=node_mask, edge_mask=-edge_mask)\n",
    "\n",
    "    def supports(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _train(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        x: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        *,\n",
    "        target: Tensor,\n",
    "        index: Optional[Union[int, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        with torch.no_grad(): self.infl_orig = sum(model(x, edge_index, **kwargs)).item()\n",
    "        self.seed_size = sum(x).item()\n",
    "\n",
    "        record = []\n",
    "        \n",
    "        self._initialize_masks(x, edge_index)\n",
    "\n",
    "        parameters = []\n",
    "        if self.node_mask is not None:\n",
    "            parameters.append(self.node_mask)\n",
    "        if self.edge_mask is not None:\n",
    "            set_masks(model, self.edge_mask, edge_index, apply_sigmoid=True)\n",
    "            parameters.append(self.edge_mask)\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: self.lr_gamma)\n",
    "\n",
    "        for i in range(1,self.epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            h = x if self.node_mask is None else x * self.node_mask.sigmoid()\n",
    "            y_hat, y = model(h, edge_index, **kwargs), target\n",
    "\n",
    "            if index is not None:\n",
    "                y_hat, y = y_hat[index], y[index]\n",
    "\n",
    "            loss, pred, size, ent = self._loss(y_hat, y, i)\n",
    "            record.append((i,loss.item(),pred.item(),size.item(),ent.item()))\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(parameters, 0.01)\n",
    "            optimizer.step()\n",
    "\n",
    "            # In the first iteration, we collect the nodes and edges that are\n",
    "            # involved into making the prediction. These are all the nodes and\n",
    "            # edges with gradient != 0 (without regularization applied).\n",
    "            if i == 0 and self.node_mask is not None:\n",
    "                if self.node_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for node \"\n",
    "                                     \"features. Please make sure that node \"\n",
    "                                     \"features are used inside the model or \"\n",
    "                                     \"disable it via `node_mask_type=None`.\")\n",
    "                self.hard_node_mask = self.node_mask.grad != 0.0\n",
    "            if i == 0 and self.edge_mask is not None:\n",
    "                if self.edge_mask.grad is None:\n",
    "                    raise ValueError(\"Could not compute gradients for edges. \"\n",
    "                                     \"Please make sure that edges are used \"\n",
    "                                     \"via message passing inside the model or \"\n",
    "                                     \"disable it via `edge_mask_type=None`.\")\n",
    "                self.hard_edge_mask = self.edge_mask.grad != 0.0\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if self.log_filename :\n",
    "            with open(self.log_filename,'w') as f:\n",
    "                f.write('seed infl_orig\\n')\n",
    "                f.write(f'{self.seed_size} {self.infl_orig}\\n')\n",
    "                f.write('epoch pred size ent loss\\n')\n",
    "                for epoch, loss, pred, size, ent in record:\n",
    "                    f.write(f'{epoch} {pred} {size} {ent} {loss}\\n')\n",
    "            mask = self.edge_mask.cpu().detach().sigmoid().numpy()\n",
    "            with open(self.log_filename[:-4]+'.npy', 'wb') as f: np.save(f, mask)\n",
    "\n",
    "    def _initialize_masks(self, x: Tensor, edge_index: Tensor):\n",
    "        node_mask_type = self.explainer_config.node_mask_type\n",
    "        edge_mask_type = self.explainer_config.edge_mask_type\n",
    "\n",
    "        device = x.device\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "        self.m = E\n",
    "        self.n = N\n",
    "\n",
    "        std = 0.1\n",
    "        if node_mask_type is None:\n",
    "            self.node_mask = None\n",
    "        elif node_mask_type == MaskType.object:\n",
    "            self.node_mask = Parameter(torch.randn(N, 1, device=device) * std)\n",
    "        elif node_mask_type == MaskType.attributes:\n",
    "            self.node_mask = Parameter(torch.randn(N, F, device=device) * std)\n",
    "        elif node_mask_type == MaskType.common_attributes:\n",
    "            self.node_mask = Parameter(torch.randn(1, F, device=device) * std)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        if edge_mask_type is None:\n",
    "            self.edge_mask = None\n",
    "        elif edge_mask_type == MaskType.object:\n",
    "            self.edge_mask = torch.nn.Parameter(torch.ones(E, requires_grad=True, device=device) * 3)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def _loss(self, y_hat: Tensor, y: Tensor, epoch) -> Tensor:\n",
    "        pred = y_hat[0]\n",
    "\n",
    "        mask = self.edge_mask[self.hard_edge_mask].sigmoid()\n",
    "        size = self.m-mask.sum()\n",
    "\n",
    "        ent = -mask * torch.log(mask + self.eps) - (1 - mask) * torch.log(1 - mask + self.eps)\n",
    "        ent = ent.mean()\n",
    "\n",
    "        loss = -(self.infl_orig-pred)/(self.infl_orig-self.seed_size) + self.size_coeff*(size/self.del_edge_num-1)**2 + self.ent_coeff*ent\n",
    "\n",
    "        return loss, pred, size, ent\n",
    "\n",
    "    def _clean_model(self, model):\n",
    "        clear_masks(model)\n",
    "        self.node_mask = self.hard_node_mask = None\n",
    "        self.edge_mask = self.hard_edge_mask = None\n",
    "\n",
    "\n",
    "def RumorGuard_O(adj_list, seed_idx, prob, del_edge_num, model_name=None, gnn_latent_dim=[128,128,128,128,128,128], gpu_num='cpu', **kwargs):\n",
    "    time_alg = -time.time()\n",
    "    kwargs['del_edge_num'] = del_edge_num\n",
    "    \n",
    "    device = set_gpu(gpu_num)\n",
    "    model = load_model(model_name, device, gnn_latent_dim=gnn_latent_dim)\n",
    "\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=RumorGuard_O_class(**kwargs),\n",
    "        explanation_type='phenomenon',\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='regression',\n",
    "            task_level='graph',\n",
    "            return_type='raw'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    n = len(adj_list)\n",
    "    is_seed = np.zeros(n, dtype=int)\n",
    "    is_seed[seed_idx] = 1\n",
    "    edge_index = [[],[]]\n",
    "    edge_attr = []\n",
    "    edge = []\n",
    "    for u in range(n):\n",
    "        for v,p in adj_list[u]:\n",
    "            edge_index[0].append(u)\n",
    "            edge_index[1].append(v)\n",
    "            edge_attr.append([p])\n",
    "            edge.append((u,v,p))\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_attr = torch.tensor(edge_attr)\n",
    "    seed = torch.from_numpy(np.expand_dims(is_seed,axis=-1)).float()\n",
    "    prob = torch.from_numpy(np.expand_dims(prob,axis=-1)).float()\n",
    "    data = Data(x=seed, edge_index=edge_index, edge_attr=edge_attr, y=prob)\n",
    "    edge = np.array(edge)\n",
    "\n",
    "    data = data.to(device)\n",
    "    explanation = explainer(data.x, data.edge_index, edge_attr=data.edge_attr, target=torch.unsqueeze(torch.sum(data.y,dim=0),0))\n",
    "    mask = explanation.edge_mask.cpu()\n",
    "\n",
    "    _, indices = torch.sort(mask, descending=True)\n",
    "    edge = edge[indices.numpy()]\n",
    "\n",
    "    time_alg += time.time()\n",
    "    return edge[:del_edge_num], time_alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RumorGuard_I(adj_list, seed_idx, prob, del_edge_num, model_name=None, gnn_latent_dim=[128,128,128,128,128,128], gpu_num='cpu', **kwargs):\n",
    "    time_alg = -time.time()\n",
    "    kwargs['del_edge_num'] = del_edge_num\n",
    "    \n",
    "    device = set_gpu(gpu_num)\n",
    "    model = load_model(model_name, device, gnn_latent_dim=gnn_latent_dim)\n",
    "\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=CaptumExplainer('Saliency'),\n",
    "        explanation_type='phenomenon',\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='regression',\n",
    "            task_level='graph',\n",
    "            return_type='raw',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    n = len(adj_list)\n",
    "    is_seed = np.zeros(n, dtype=int)\n",
    "    is_seed[seed_idx] = 1\n",
    "    edge_index = [[],[]]\n",
    "    edge_attr = []\n",
    "    edge = []\n",
    "    for u in range(n):\n",
    "        for v,p in adj_list[u]:\n",
    "            edge_index[0].append(u)\n",
    "            edge_index[1].append(v)\n",
    "            edge_attr.append([p])\n",
    "            edge.append((u,v,p))\n",
    "    edge_index = torch.tensor(edge_index)\n",
    "    edge_attr = torch.tensor(edge_attr)\n",
    "    seed = torch.from_numpy(np.expand_dims(is_seed,axis=-1)).float()\n",
    "    prob = torch.from_numpy(np.expand_dims(prob,axis=-1)).float()\n",
    "    data = Data(x=seed, edge_index=edge_index, edge_attr=edge_attr, y=prob)\n",
    "    edge = np.array(edge)\n",
    "\n",
    "    data = data.to(device)\n",
    "    explanation = explainer(data.x, data.edge_index, edge_attr=data.edge_attr, target=torch.unsqueeze(torch.sum(data.y,dim=0),0))\n",
    "    mask = explanation.edge_mask.cpu()\n",
    "\n",
    "    if 'prod_p' in kwargs and kwargs['prod_p']==True:\n",
    "        mask = mask * edge_attr.cpu().squeeze()\n",
    "\n",
    "    _, indices = torch.sort(mask, descending=True)\n",
    "    edge = edge[indices.numpy()]\n",
    "\n",
    "    time_alg += time.time()\n",
    "    return edge[:del_edge_num], time_alg"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
